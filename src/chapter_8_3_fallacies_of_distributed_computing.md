# Fallacies of Distributed Computing

在理想的情况下，两台电脑之间通过网络通信时，所有的事情都会正常的进行：一个处理器打开一个连接，发送他的数据，得到响应，皆大欢喜。假设这个操作总是能够成功执行并且不存在任何能够产生危险的错误，当某些情况出现并导致我们的假设出错时，就会很难或不可能去预测整个系统的行为了。

大部分时间里，都假设网络是可靠的，至少在某种程度上是可靠的，才能保证他是有用的。我们都遇到过，在尝试建立与远程服务的网络连接时，得到了一个网络不可触达的错误 *(Network is Unreachable)* 。 但就算跟远程服务的连接成功建立后，这个成功初始化并建立的连接并不能保证他后续的可靠性，这个连接可能会在任意的时刻被中断。发送的消息也许能够成功的发送给对端，但对应的响应信息页可能会丢失，或者对方发送了响应但在消息到达前连接被中断。

交换机出错、电缆断开、网络配置可能在任意时刻发生变更，我们在构建系统时需要使用合适的方式来处理这些场景。

连接可以是稳定的，但我们不能指望远程的调用会跟本地的调用一样快，我们要假设调用通常是会有延迟的，并且延迟不可能为零。消息需要通过多个软件层级，以及类似光纤、电缆才能到达远程的服务，而所有的这些操作都不是能够即时完成的。

Michael Lewis 在他 *Flash Boys* 这本书中讲了个故事，有一个公司花了数百万美元降低了几毫秒的延迟，从而能够比竞争对手更快访问证券交易所。这是一个使用降低延迟来提高竞争力的很好的例子，只是有一点是需要提一下的，根据一些类似如 BARTLETT16 的研究，更快的知道价格并去执行订单并不能给交易员带来更大的获利机会。

在接下来的学习中，我们还会添加入重试、重新连接跟删除我们关于瞬时实行的假设，但这仍然被证明是不够的。当增加了交换信息的数量、频率跟大小或者在网络中增加了新的处理器时，我们不能够假设 *Bandwidth is infinite* 带宽是无限的。

Deutsch 有一份非常详尽的分布式计算谬论列表，只是他关注的是消息在网络中从一个处理器发送另外一个处理器。这其中关注的大部分是最通用跟最底层的复杂情况，但很不幸的是，在我们后面关于分布式系统设计跟实现时还做了很多其他的假设，这些假设在执行时可能也会导致问题。

## Processing

在远程的处理器为刚接收到的消息发送响应前，他需要在他的本地执行一些工作，因此我们不能够假设处理的过程是能够瞬时完成的。仅仅考虑网络的延迟并不足够，因为远程处理器所执行的操作也不是能够马上完成的。

而且，也没有任何保证说对方会在接收到消息的同时就马上进行处理。消息可能会在远程服务的等待队列中排队，一直等到在他之前的消息都完成之后才开始处理。

可以定位出节点之间距离的远近，确认他们是否有不同的 CPU 芯片、内存大小、不同的硬盘或是否以不同的配置运行在不同版本的软件上。但我们不能期望他们能够以相同的速率来处理请求，如果我们有一个任务需要等待多个远程的服务并行运行的响应，那整个执行的时间最多将会跟最慢的那个远程服务器一样。

跟通常的认知相反，队列的容量并不是无限的，而且将更多的请求进行排队对系统来说没有什么好处。*Backpressure* 背压是一个允许我们处理生产者生成速度快于消费者消费速度的策略。背压是分布式系统中最不为人知的概念之一，他通常是在事后建立的，而不是在系统设计时考虑的部分。

尽管提高队列的容量听起来是一个对 *Pipeline* 管道、*Parallelize* 并行化跟高效调度请求很好的想法，好像让更多的消息在队列中排队等待运行并不会带来任何影响。但增加队列的大小是可能会对延迟带来负面影响的，因为对他的变更对处理的速度没任何影响。

通常来说，处理本地的队列是为了实现下面的目标：

- *Decoupling* 解耦

  将接收跟处理在时间上分开，并让他们独立的运行。

- *Pipilining* 管道

  处于不同阶段的请求可以被系统中独立的部分去处理。负责接收消息的子系统并不需要等到之前的所有消息都被完全处理才能继续执行。

- *Absorbing short-time bursts* 处理短期爆发

  系统的负载总是变化的，能够将请求到达的时间对负责处理请求并提供响应的模块隐藏起来。系统整体的延迟会因为将时间花费在排队上变得更高，但在大部分情况下相比于与返回失败跟重试来说又会更好一些。

队列的大小在不同的负载跟应用中是不一样的，为了得到稳定的负载，我们可以先确认任务处理所需的时间以及任务在队列中等待所需花费的平均时间，来确保在吞吐量上升的时候整体的延迟仍在可接受的范围内。在这个例子中，队列的大小会相对较小。对于无法预测的负载，当任务是以突发的方式提交时，队列大小的设置也应该将突发和高负载一同列入考虑。

远程的服务器可以快速的处理请求，但这不意味着我们可以一直从中得到正面的响应，他可以响应一个失败的信息：比如无法进行写入，检索的数据没找到，也可能是因为触发了 Bug。总而言之，尽管是最乐观的场景我们也应该对他进行关注。

## Clocks and Time

> Time is an illusion. Lunchtime doubly so.
> 	-- Ford Prefect, *The Hitchhiker's Guide to the Galaxy*

假设远程服务器之间的时钟是同步的是一件很危险的事。结合延迟为零跟处理是即时完成的这两点，这把我们导向了一种不同的特性，特别是在时间序列跟实时数据处理中。比如在从具有不同时间认知的参与者中收集跟分析数据时，你应该了解他们之间的时间偏差跟做一些相应的时间规范，而不是依赖于时间源的时间戳信息。除非你使用专门的高精度时间源，否则的话就不应该让同步跟时序依赖于时间戳信息。当然这并不意味着我们没办法或应该完全不依赖于时间：毕竟任意的同步系统都会使用 *local* 本地的时钟来处理超时。

我们需要时刻考虑不同处理器之间可能存在的时间差异以及消息送达跟处理所需的时间。比如 Spanner *(Distributed Transactions with Spanner 会提到)*  使用一种特殊的时间 API 来返回时间戳以及不确定的边界来明确事务间的时序问题。还有一些错误检测算法会依赖于时间共享的概念来保证时钟的偏差总会在能够保证正确性的边界内。

除此之外，实际上很难在分布式系统中实现时钟的同步，*current* 当前时间会持续的在变化：你可以从操作系统提供的 POSIX 接口中请求到当前时间戳，然后在执行几个步骤后请求另一个当前时间戳，这会得到两个不同的值。这是一个相对直观的现象，但要理解事件源跟具体时刻所获取的时间戳是至关紧要的。

了解时钟源是严格递增的 *(比如不会回退)* 跟被调度的与时间相关的操作之间的偏差可能会多大也是非常有用的。

## State Consistency

我们之前所作的大部分假设属于在大多数情况下总会出错的范畴，但其中也有一些能被较好的描述为并不总是正确的：他们只是使用了简化的模型、特定的思维方式以及忽略了一些复杂的边界条件来让其更容易理解。

分布式算法并不总是保证严格的状态一致性。有一些实现具有较松限制来允许不同部分之间的状态有



















































